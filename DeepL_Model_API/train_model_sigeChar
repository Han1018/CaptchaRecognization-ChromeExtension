{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_model","provenance":[],"collapsed_sections":[],"mount_file_id":"1vDQLWCGYAb8CjDnzP-lmjo9JrWPxXBls","authorship_tag":"ABX9TyNkJYDtr7E7nncnrqHWEdxC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VFC1HF9qhUdK"},"source":["import matplotlib.pyplot as plt\r\n","import os\r\n","import time\r\n","import numpy as np  \r\n","import tensorflow as tf\r\n","import numpy as np  \r\n","from PIL import Image  \r\n","from keras.callbacks import ModelCheckpoint,TensorBoard\r\n","from keras.models import Sequential,Model\r\n","from keras.layers import Dense, Dropout, Flatten, Conv2D,Conv1D, MaxPooling2D,Reshape,BatchNormalization\r\n","from keras.utils import to_categorical\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.preprocessing import StandardScaler\r\n","import keras\r\n","import glob \r\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mEh5JcpjSND"},"source":["image_lenth=32\r\n","# alphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']  \r\n","ALPHABET = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\r\n","all_set=ALPHABET#+alphabet\r\n","dict_all={}\r\n","\r\n","for i, c in enumerate(all_set):\r\n","  dict_all[c]=i\r\n","\r\n","CHAR_SET_LEN=len(all_set)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Mvvh6lkjhAx"},"source":["def text2vec(text):  \r\n","#  One-hot 編碼\r\n","#    src:https://blog.csdn.net/cskywit/article/details/79230658\r\n","  #one-hot 格式 (52,1)#1是看有幾個字 這裡是1爾以\r\n","  vector = np.zeros([CHAR_SET_LEN])  \r\n","  for i, c in enumerate(text):  \r\n","      idx = i * CHAR_SET_LEN + dict_all[c]\r\n","      vector[idx] = 1  \r\n","      # vector[i,dict_all[c]] = 1  \r\n","  return vector "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXqHb-QpiReN"},"source":["def get_file_data():\r\n","  X_train=[]\r\n","  Y_train=[]\r\n","  i=0\r\n","  big_ALPHABAT='/content/drive/MyDrive/ProJ_Proofnum/splited_pic_big/'\r\n","  small_alphabat='/content/drive/MyDrive/ProJ_Proofnum/splited_pic_small/'\r\n","\r\n","  #讀取大寫的圖片\r\n","  for i,label in enumerate(ALPHABET):\r\n","      i=0\r\n","      for file_image in glob.glob(big_ALPHABAT+label+'/*.jpg'):\r\n","          i=i+1\r\n","          if(i%100==0):\r\n","            print(label,i)\r\n","          X_train.append(np.array(Image.open(file_image)).reshape(32,32,1))\r\n","          Y_train.append(text2vec(label))\r\n","  print('Collected big data')\r\n","\r\n","  # #讀取小寫的圖片\r\n","  # for i,label in enumerate(alphabet):\r\n","  #     i=0\r\n","  #     for file_image in glob.glob(small_alphabat+label+'/*.jpg'):\r\n","  #         i=i+1\r\n","  #         if(i%50==0):\r\n","  #           print(label,i)\r\n","  #         X_train.append(np.array(Image.open(file_image)).reshape(32,32,1))\r\n","  #         Y_train.append(text2vec(label))\r\n","  # print('Collected small data')\r\n","  X_train=np.array(X_train)\r\n","  Y_train=np.array(Y_train)\r\n","  return X_train,Y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaAIL4HOmxLL"},"source":["def split_train_data(x_train,y_train):\r\n","    # scaler = StandardScaler()\r\n","    # x_train=scaler.fit_transform(x_train)\r\n","    X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size = 0.3)\r\n","    return X_train, X_test, Y_train, Y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TygZ-A3nf97"},"source":["def create_model(input_shape):\r\n","  inp = keras.Input(input_shape)\r\n","  out = inp\r\n","  \r\n","  out = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(out)\r\n","  out = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(out)\r\n","  out = BatchNormalization()(out)\r\n","  out = MaxPooling2D(pool_size=(2, 2))(out)\r\n","  out = Dropout(0.25)(out)\r\n","\r\n","  out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)\r\n","  out = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(out)\r\n","  out = BatchNormalization()(out)\r\n","  out = MaxPooling2D(pool_size=(2, 2))(out)\r\n","  out = Dropout(0.25)(out)\r\n","\r\n","  out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(out)\r\n","  out = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')(out)\r\n","  out = BatchNormalization()(out)\r\n","  out = MaxPooling2D(pool_size=(2, 2))(out)\r\n","  out = Dropout(0.25)(out)\r\n","\r\n","  # out = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')(out)\r\n","  # out = BatchNormalization()(out)\r\n","  # out = MaxPooling2D(pool_size=(2, 2))(out)\r\n","\r\n","  out = Flatten()(out)\r\n","  out = Dropout(0.25)(out)\r\n","  out = Dense(len(all_set), name='digit1', activation='softmax')(out)\r\n","\r\n","  model = Model(inputs=inp, outputs=out)\r\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuuY-irilO4o"},"source":["def main():\r\n","  batch_size=10\r\n","  #抓取每個資料夾下的圖片 並進行one-hot decode\r\n","  X_train,Y_train = get_file_data()\r\n","  print(X_train.shape)\r\n","  #normallize並產生訓練&test資料\r\n","  X_train, X_test, Y_train, Y_test=split_train_data(X_train,Y_train)\r\n","  callbacks_list=[]\r\n","  #建立model\r\n","  model = create_model(X_train[0].shape)\r\n","\r\n","  #trainning\r\n","  model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\r\n","\r\n","  # checkpoint\r\n","  filepath=\"/content/drive/MyDrive/ProJ_Proofnum/weights.best.hdf5\"\r\n","  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\r\n","  mode='max')\r\n","  LOG_DIR=os.path.join(\"/content/drive/MyDrive/ProJ_Proofnum/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n","  tbCallBack = TensorBoard(log_dir=LOG_DIR, \r\n","                         histogram_freq=1,\r\n","                         write_graph=True,\r\n","                         write_grads=True,\r\n","                         batch_size=batch_size,\r\n","                         write_images=True)\r\n","  \r\n","  callbacks_list = [checkpoint,tbCallBack]\r\n","\r\n","  #run\r\n","  model.fit(X_train, Y_train, batch_size=10, epochs=500, verbose=1, validation_data=(X_test, Y_test),callbacks=callbacks_list)\r\n","  score = model.evaluate(X_test, Y_test,verbose=1)\r\n","  \r\n","  # 模型存檔\r\n","  from keras.models import load_model\r\n","  model.save('/content/drive/MyDrive/ProJ_Proofnum/ASR.h5')  # creates a HDF5 file 'model.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5xcGPWilO2r"},"source":["\r\n","main()"],"execution_count":null,"outputs":[]}]}